# Fine-tuning Configuration for SmolLM3-3B with LoRA + NVFP4
# =========================================================

# Model Configuration
model:
  name: "HuggingFaceTB/SmolLM3-3B"
  trust_remote_code: true
  attn_implementation: "sdpa"

# Quantization (NVFP4)
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "fp4"  # NVFP4 format
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

# LoRA Configuration
lora:
  r: 64
  lora_alpha: 128
  lora_dropout: 0.05
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# Dataset
dataset:
  name: "TeichAI/claude-4.5-opus-high-reasoning-250x"
  split: "train"

# Training Configuration
training:
  output_dir: "./output/smollm3-3b-reasoning-lora"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  max_seq_length: 8192
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"
  bf16: true
  tf32: true
  max_grad_norm: 0.3
  logging_steps: 1
  save_strategy: "epoch"
  save_total_limit: 2
  seed: 42
